{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":14215481,"sourceType":"datasetVersion","datasetId":9067795},{"sourceId":287108030,"sourceType":"kernelVersion"}],"dockerImageVersionId":31240,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install torch-geometric\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# =============================================================\n# FULL END-TO-END DDI SYSTEM (KAGGLE-READY)\n# GNN + MC Dropout + Reliability Auditor + Evaluation + Saving\n# Author: Gaurav\n# =============================================================\n\n# =============================================================\n# 0. IMPORTS & GLOBAL CONFIG (KAGGLE SAFE)\n# =============================================================\nimport warnings, os, json, joblib, random\nwarnings.filterwarnings(\"ignore\")\n\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nfrom collections import Counter, defaultdict\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.optim import AdamW\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nfrom torch.cuda.amp import autocast, GradScaler\n\nfrom torch_geometric.data import Data, Batch\nfrom torch_geometric.loader import DataLoader\nfrom torch_geometric.nn import GATv2Conv, global_mean_pool, global_max_pool, LayerNorm\n\nfrom rdkit import Chem\nfrom rdkit.Chem import rdchem, AllChem\nfrom rdkit.Chem.Scaffolds import MurckoScaffold\n\nfrom sklearn.metrics import accuracy_score, f1_score, classification_report\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom xgboost import XGBClassifier\n\n\n# =============================================================\n# EMA HELPER (MEDICAL-GRADE STABILITY) – KEEP ONLY ONCE\n# =============================================================\n\nclass EMA:\n    def __init__(self, model, decay=0.999):\n        self.decay = decay\n        self.shadow = {}\n        self.backup = {}\n\n        for name, param in model.named_parameters():\n            if param.requires_grad:\n                self.shadow[name] = param.data.clone()\n\n    def update(self, model):\n        for name, param in model.named_parameters():\n            if param.requires_grad:\n                self.shadow[name] = (\n                    self.decay * self.shadow[name]\n                    + (1.0 - self.decay) * param.data\n                )\n\n    def apply_shadow(self, model):\n        for name, param in model.named_parameters():\n            if param.requires_grad:\n                self.backup[name] = param.data.clone()\n                param.data = self.shadow[name]\n\n    def restore(self, model):\n        for name, param in model.named_parameters():\n            if param.requires_grad:\n                param.data = self.backup[name]\n        self.backup = {}\n\n# Performance flags\ntorch.backends.cudnn.benchmark = True\ntorch.set_float32_matmul_precision(\"high\")\n\n# Reproducibility\ndef set_seed(seed=42):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n\nset_seed(42)\n\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Device:\", DEVICE)\n\nFILE_PATH = \"/kaggle/input/drugbank3/drugbank.tab\"   \ntorch.backends.cudnn.benchmark = False\ntorch.backends.cudnn.deterministic = True","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-18T19:51:13.476432Z","iopub.execute_input":"2025-12-18T19:51:13.477023Z","iopub.status.idle":"2025-12-18T19:51:13.488913Z","shell.execute_reply.started":"2025-12-18T19:51:13.476999Z","shell.execute_reply":"2025-12-18T19:51:13.488136Z"}},"outputs":[{"name":"stdout","text":"Device: cuda\n","output_type":"stream"}],"execution_count":48},{"cell_type":"code","source":"import torch\nimport torch_geometric\n\nprint(\"Torch version:\", torch.__version__)\nprint(\"PyG version:\", torch_geometric.__version__)\nprint(\"CUDA available:\", torch.cuda.is_available())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-18T19:44:42.739318Z","iopub.execute_input":"2025-12-18T19:44:42.739831Z","iopub.status.idle":"2025-12-18T19:44:42.759875Z","shell.execute_reply.started":"2025-12-18T19:44:42.739813Z","shell.execute_reply":"2025-12-18T19:44:42.759182Z"}},"outputs":[{"name":"stdout","text":"ERROR! Session/line number was not unique in database. History logging moved to new session 30\nTorch version: 2.6.0+cu124\nPyG version: 2.7.0\nCUDA available: True\n","output_type":"stream"}],"execution_count":38},{"cell_type":"code","source":"# =============================================================\n# 1. LOAD & CLEAN DATASET\n# =============================================================\ndf = pd.read_csv(FILE_PATH, sep=\"\\t\")\nprint(\"Raw samples:\", len(df))\nprint(\"Raw classes:\", df['Y'].nunique())\n\n# Convert labels to 0-based\nif df['Y'].min() >= 1:\n    df['Y'] -= 1\n\n# Remove rare classes\nMIN_SAMPLES = 20\nvalid = df['Y'].value_counts()\nvalid = valid[valid >= MIN_SAMPLES].index\ndf = df[df['Y'].isin(valid)].copy()\n\n# Relabel\nlabel_map = {old: new for new, old in enumerate(sorted(df['Y'].unique()))}\ndf['Y'] = df['Y'].map(label_map)\nNUM_CLASSES = df['Y'].nunique()\n\nprint(\"Filtered samples:\", len(df))\nprint(\"Final classes:\", NUM_CLASSES)\n\n# Validate SMILES\ndef valid_smiles(s):\n    return Chem.MolFromSmiles(str(s)) is not None\n\ndf = df[df['X1'].apply(valid_smiles) & df['X2'].apply(valid_smiles)]\ndf.reset_index(drop=True, inplace=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-18T19:44:42.760621Z","iopub.execute_input":"2025-12-18T19:44:42.761222Z","iopub.status.idle":"2025-12-18T19:45:41.663612Z","shell.execute_reply.started":"2025-12-18T19:44:42.761206Z","shell.execute_reply":"2025-12-18T19:45:41.663016Z"}},"outputs":[{"name":"stdout","text":"Raw samples: 191808\nRaw classes: 86\nFiltered samples: 191700\nFinal classes: 76\n","output_type":"stream"}],"execution_count":39},{"cell_type":"code","source":"# =============================================================\n# 2. SCAFFOLD SPLIT (NO CHEMICAL LEAKAGE)\n# =============================================================\nprint(\"Performing Scaffold Split (Scientific Best Practice)...\")\ndef get_scaffold(smiles):\n    mol = Chem.MolFromSmiles(str(smiles))\n    if mol is None: return None\n    return MurckoScaffold.MurckoScaffoldSmiles(mol=mol)\n\nscaffold_groups = defaultdict(list)\nfor idx, row in tqdm(df.iterrows(), total=len(df)):\n    scaf = get_scaffold(row.X1)\n    if scaf: scaffold_groups[scaf].append(idx)\n\nscaffolds = sorted(scaffold_groups.keys(), key=lambda x: len(scaffold_groups[x]), reverse=True)\ntrain_idx, test_idx = [], []\n\nfor scaf in scaffolds:\n    if len(train_idx) < 0.8 * len(df): train_idx.extend(scaffold_groups[scaf])\n    else: test_idx.extend(scaffold_groups[scaf])\n\ntrain_df = df.loc[train_idx].reset_index(drop=True)\ntest_df  = df.loc[test_idx].reset_index(drop=True)\n\nprint(\"Train:\", len(train_df), \"Test:\", len(test_df))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-18T19:45:41.664962Z","iopub.execute_input":"2025-12-18T19:45:41.665273Z","iopub.status.idle":"2025-12-18T19:47:21.631712Z","shell.execute_reply.started":"2025-12-18T19:45:41.665253Z","shell.execute_reply":"2025-12-18T19:47:21.630799Z"}},"outputs":[{"name":"stdout","text":"Performing Scaffold Split (Scientific Best Practice)...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 191700/191700 [01:39<00:00, 1918.94it/s]","output_type":"stream"},{"name":"stdout","text":"Train: 153395 Test: 29323\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":40},{"cell_type":"code","source":"# =============================================================\n# 3. GRAPH CONSTRUCTION\n# =============================================================\ndef atom_features(atom):\n    hyb = {\n        rdchem.HybridizationType.SP: 0,\n        rdchem.HybridizationType.SP2: 1,\n        rdchem.HybridizationType.SP3: 2,\n        rdchem.HybridizationType.SP3D: 3,\n        rdchem.HybridizationType.SP3D2: 4,\n    }\n    return [\n        atom.GetAtomicNum(), atom.GetTotalDegree(),\n        hyb.get(atom.GetHybridization(), 5),\n        int(atom.GetIsAromatic()), atom.GetFormalCharge(),\n        int(atom.IsInRing()), atom.GetNumRadicalElectrons(),\n        int(atom.GetChiralTag())\n    ]\n\ndef bond_features(bond):\n    bt = {\n        rdchem.BondType.SINGLE: 0,\n        rdchem.BondType.DOUBLE: 1,\n        rdchem.BondType.TRIPLE: 2,\n        rdchem.BondType.AROMATIC: 3,\n    }\n    return [bt.get(bond.GetBondType(), 4), int(bond.GetIsConjugated()), int(bond.IsInRing())]\n\ndef smiles_to_graph(smiles):\n    mol = Chem.MolFromSmiles(str(smiles))\n    if mol is None: return None\n    x = torch.tensor([atom_features(a) for a in mol.GetAtoms()], dtype=torch.float)\n    ei, ea = [], []\n    for b in mol.GetBonds():\n        i, j = b.GetBeginAtomIdx(), b.GetEndAtomIdx()\n        ei += [[i, j], [j, i]]\n        f = bond_features(b)\n        ea += [f, f]\n    if len(ei) == 0:\n        ei = torch.empty((2, 0), dtype=torch.long)\n        ea = torch.empty((0, 3), dtype=torch.float)\n    else:\n        ei = torch.tensor(ei).t().contiguous()\n        ea = torch.tensor(ea, dtype=torch.float)\n    return Data(x=x, edge_index=ei, edge_attr=ea)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-18T19:47:21.632616Z","iopub.execute_input":"2025-12-18T19:47:21.632935Z","iopub.status.idle":"2025-12-18T19:47:21.641206Z","shell.execute_reply.started":"2025-12-18T19:47:21.632910Z","shell.execute_reply":"2025-12-18T19:47:21.640402Z"}},"outputs":[],"execution_count":41},{"cell_type":"code","source":"# =============================================================\n# 4. DATASET & LOADERS \n# =============================================================\n\nMICRO_BATCH = 32        \nACCUM_STEPS = 4        \n\nclass DDIDataset(torch.utils.data.Dataset):\n    def __init__(self, df):\n        self.df = df.reset_index(drop=True)\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        r = self.df.iloc[idx]\n        g1 = smiles_to_graph(r.X1)\n        g2 = smiles_to_graph(r.X2)\n        if g1 is None or g2 is None:\n            return None\n        return g1, g2, torch.tensor(r.Y, dtype=torch.long)\n\ndef collate(batch):\n    batch = [b for b in batch if b is not None]\n    if not batch:\n        return None\n    g1, g2, y = zip(*batch)\n    return (\n        Batch.from_data_list(g1),\n        Batch.from_data_list(g2),\n        torch.stack(y)\n    )\n\ntrain_loader = DataLoader(\n    DDIDataset(train_df),\n    batch_size=MICRO_BATCH,\n    shuffle=True,\n    collate_fn=collate,\n    pin_memory=True,\n    num_workers=2\n)\n\ntest_loader = DataLoader(\n    DDIDataset(test_df),\n    batch_size=MICRO_BATCH,\n    shuffle=False,\n    collate_fn=collate,\n    pin_memory=True,\n    num_workers=2\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-18T19:47:21.641967Z","iopub.execute_input":"2025-12-18T19:47:21.642184Z","iopub.status.idle":"2025-12-18T19:47:21.665891Z","shell.execute_reply.started":"2025-12-18T19:47:21.642161Z","shell.execute_reply":"2025-12-18T19:47:21.665160Z"}},"outputs":[],"execution_count":42},{"cell_type":"code","source":"# =============================================================\n# 5. GNN MODEL \n# =============================================================\n\nclass Encoder(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.c1 = GATv2Conv(8, 128, heads=4, edge_dim=3)\n        self.n1 = LayerNorm(512)\n        self.c2 = GATv2Conv(512, 128, heads=2, edge_dim=3)\n        self.n2 = LayerNorm(256)\n        self.c3 = GATv2Conv(256, 128, heads=1, edge_dim=3)\n        self.n3 = LayerNorm(128)\n        self.dp = nn.Dropout(0.25)\n\n    def forward(self, g):\n        x, ei, ea, b = g.x, g.edge_index, g.edge_attr, g.batch\n        x = self.dp(F.elu(self.n1(self.c1(x, ei, ea))))\n        x = self.dp(F.elu(self.n2(self.c2(x, ei, ea))))\n        x = F.elu(self.n3(self.c3(x, ei, ea)))\n        return torch.cat(\n            [global_mean_pool(x, b), global_max_pool(x, b)],\n            dim=1\n        )\n\nclass SiameseDDI(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.enc = Encoder()\n        self.fc = nn.Sequential(\n            nn.Linear(512, 256),\n            nn.LayerNorm(256),\n            nn.ELU(),\n            nn.Dropout(0.3),\n            nn.Linear(256, 128),\n            nn.ELU(),\n            nn.Linear(128, NUM_CLASSES)\n        )\n\n    def forward(self, g1, g2):\n        return self.fc(\n            torch.cat([self.enc(g1), self.enc(g2)], dim=1)\n        )\n\n\nmodel = SiameseDDI().to(DEVICE)\nema = EMA(model, decay=0.999)\n\nprint(\"Running on SINGLE GPU (PyG safe)\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-18T19:47:21.666517Z","iopub.execute_input":"2025-12-18T19:47:21.666739Z","iopub.status.idle":"2025-12-18T19:47:21.692256Z","shell.execute_reply.started":"2025-12-18T19:47:21.666725Z","shell.execute_reply":"2025-12-18T19:47:21.691620Z"}},"outputs":[{"name":"stdout","text":"Running on SINGLE GPU (PyG safe)\n","output_type":"stream"}],"execution_count":43},{"cell_type":"code","source":"# =============================================================\n# 6. TRAINING SETUP (FINAL – MEDICAL GRADE)\n# =============================================================\n\n# ---- CLASS WEIGHTS (FIXED MISMATCH SAFE) ----\ntrain_classes = np.unique(train_df.Y)\n\ncomputed_weights = compute_class_weight(\n    class_weight=\"balanced\",\n    classes=train_classes,\n    y=train_df.Y\n)\n\nfull_weights = np.ones(NUM_CLASSES)\nfull_weights[train_classes] = computed_weights\nweights = torch.tensor(np.sqrt(full_weights), dtype=torch.float).to(DEVICE)\n\nclass FocalLoss(nn.Module):\n    def forward(self, x, y):\n        ce = F.cross_entropy(x, y, weight=weights, reduction=\"none\")\n        pt = torch.exp(-ce)\n        return (((1 - pt) ** 2) * ce).mean()\n\ncriterion = FocalLoss()\n\nBASE_LR = 3e-4       \noptimizer = AdamW(\n    model.parameters(),\n    lr=BASE_LR,\n    weight_decay=1e-4\n)\n\n# ---- LR SCHEDULER ----\nscheduler = ReduceLROnPlateau(\n    optimizer,\n    mode=\"max\",\n    patience=3,\n    factor=0.5\n)\n\n# ---- AMP ----\nscaler = GradScaler(enabled=(DEVICE.type == \"cuda\"))\n\n# ---- WARMUP ----\nWARMUP_EPOCHS = 2\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-18T19:51:41.280127Z","iopub.execute_input":"2025-12-18T19:51:41.280657Z","iopub.status.idle":"2025-12-18T19:51:41.317918Z","shell.execute_reply.started":"2025-12-18T19:51:41.280617Z","shell.execute_reply":"2025-12-18T19:51:41.317184Z"}},"outputs":[],"execution_count":49},{"cell_type":"code","source":"# =============================================================\n# 7. TRAINING LOOP (FINAL - WITH PROPER EMA VALIDATION)\n# =============================================================\n\nbest_f1, wait = 0, 0\nEPOCHS = 30\n\nfor ep in range(EPOCHS):\n    # ---- LR WARMUP ----\n    if ep < WARMUP_EPOCHS:\n        lr_scale = (ep + 1) / WARMUP_EPOCHS\n        for pg in optimizer.param_groups:\n            pg[\"lr\"] = BASE_LR * lr_scale\n\n    model.train()\n    optimizer.zero_grad()\n    total_loss = 0\n\n    for step, batch in enumerate(tqdm(train_loader, desc=f\"Epoch {ep+1}\")):\n        if batch is None: continue\n\n        g1, g2, y = batch\n        g1, g2, y = g1.to(DEVICE), g2.to(DEVICE), y.to(DEVICE)\n\n        with autocast(enabled=(DEVICE.type == \"cuda\")):\n            out = model(g1, g2)\n            loss = criterion(out, y) / ACCUM_STEPS\n\n        scaler.scale(loss).backward()\n        total_loss += loss.item()\n\n        if (step + 1) % ACCUM_STEPS == 0:\n            scaler.unscale_(optimizer)\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n            scaler.step(optimizer)\n            scaler.update()\n            optimizer.zero_grad()\n            ema.update(model) \n    # ---------- VALIDATION (USING EMA WEIGHTS) ----------\n    ema.apply_shadow(model) \n    \n    model.eval()\n    yt, yp = [], []\n\n    with torch.no_grad():\n        for batch in test_loader:\n            if batch is None: continue\n            g1, g2, y = batch\n            out = model(g1.to(DEVICE), g2.to(DEVICE))\n            yt.extend(y.tolist())\n            yp.extend(out.argmax(dim=1).cpu().tolist())\n\n    ema.restore(model)\n\n    acc = accuracy_score(yt, yp)\n    f1  = f1_score(yt, yp, average=\"weighted\")\n\n    print(f\"Epoch {ep+1:02d} | Loss {total_loss:.2f} | Acc {acc:.4f} | F1 {f1:.4f}\")\n\n    scheduler.step(f1)\n\n    if f1 > best_f1:\n        best_f1 = f1\n        wait = 0\n        ema.apply_shadow(model)\n        torch.save(model.state_dict(), \"best_ddi_model.pt\")\n        ema.restore(model)\n    else:\n        wait += 1\n        if wait >= 6:\n            print(\"Early stopping triggered.\")\n            break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-18T19:58:33.341734Z","iopub.execute_input":"2025-12-18T19:58:33.342729Z","iopub.status.idle":"2025-12-18T22:11:50.479346Z","shell.execute_reply.started":"2025-12-18T19:58:33.342687Z","shell.execute_reply":"2025-12-18T22:11:50.478490Z"}},"outputs":[{"name":"stderr","text":"Epoch 1: 100%|██████████| 4794/4794 [03:42<00:00, 21.54it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 01 | Loss 1034.26 | Acc 0.1550 | F1 0.1549\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2: 100%|██████████| 4794/4794 [03:47<00:00, 21.09it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 02 | Loss 833.95 | Acc 0.1921 | F1 0.1893\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3: 100%|██████████| 4794/4794 [03:47<00:00, 21.09it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 03 | Loss 635.79 | Acc 0.2415 | F1 0.2427\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4: 100%|██████████| 4794/4794 [03:47<00:00, 21.07it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 04 | Loss 500.72 | Acc 0.2845 | F1 0.2876\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5: 100%|██████████| 4794/4794 [03:47<00:00, 21.06it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 05 | Loss 415.00 | Acc 0.3243 | F1 0.3271\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6: 100%|██████████| 4794/4794 [03:47<00:00, 21.10it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 06 | Loss 354.53 | Acc 0.3542 | F1 0.3580\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7: 100%|██████████| 4794/4794 [03:47<00:00, 21.07it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 07 | Loss 305.91 | Acc 0.3795 | F1 0.3823\n","output_type":"stream"},{"name":"stderr","text":"Epoch 8: 100%|██████████| 4794/4794 [03:42<00:00, 21.53it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 08 | Loss 270.81 | Acc 0.3985 | F1 0.4013\n","output_type":"stream"},{"name":"stderr","text":"Epoch 9: 100%|██████████| 4794/4794 [03:42<00:00, 21.54it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 09 | Loss 245.70 | Acc 0.4181 | F1 0.4210\n","output_type":"stream"},{"name":"stderr","text":"Epoch 10: 100%|██████████| 4794/4794 [03:42<00:00, 21.58it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 10 | Loss 225.83 | Acc 0.4336 | F1 0.4364\n","output_type":"stream"},{"name":"stderr","text":"Epoch 11: 100%|██████████| 4794/4794 [03:42<00:00, 21.57it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 11 | Loss 210.67 | Acc 0.4415 | F1 0.4437\n","output_type":"stream"},{"name":"stderr","text":"Epoch 12: 100%|██████████| 4794/4794 [03:41<00:00, 21.65it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 12 | Loss 192.77 | Acc 0.4544 | F1 0.4568\n","output_type":"stream"},{"name":"stderr","text":"Epoch 13: 100%|██████████| 4794/4794 [03:41<00:00, 21.60it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 13 | Loss 179.90 | Acc 0.4676 | F1 0.4699\n","output_type":"stream"},{"name":"stderr","text":"Epoch 14: 100%|██████████| 4794/4794 [03:42<00:00, 21.57it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 14 | Loss 167.10 | Acc 0.4772 | F1 0.4798\n","output_type":"stream"},{"name":"stderr","text":"Epoch 15: 100%|██████████| 4794/4794 [03:42<00:00, 21.58it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 15 | Loss 157.55 | Acc 0.4861 | F1 0.4890\n","output_type":"stream"},{"name":"stderr","text":"Epoch 16: 100%|██████████| 4794/4794 [03:41<00:00, 21.60it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 16 | Loss 152.51 | Acc 0.4955 | F1 0.4990\n","output_type":"stream"},{"name":"stderr","text":"Epoch 17: 100%|██████████| 4794/4794 [03:41<00:00, 21.65it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 17 | Loss 143.08 | Acc 0.5056 | F1 0.5097\n","output_type":"stream"},{"name":"stderr","text":"Epoch 18: 100%|██████████| 4794/4794 [03:42<00:00, 21.55it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 18 | Loss 138.36 | Acc 0.5108 | F1 0.5148\n","output_type":"stream"},{"name":"stderr","text":"Epoch 19: 100%|██████████| 4794/4794 [03:42<00:00, 21.57it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 19 | Loss 128.84 | Acc 0.5208 | F1 0.5249\n","output_type":"stream"},{"name":"stderr","text":"Epoch 20: 100%|██████████| 4794/4794 [03:41<00:00, 21.64it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 20 | Loss 124.05 | Acc 0.5272 | F1 0.5314\n","output_type":"stream"},{"name":"stderr","text":"Epoch 21: 100%|██████████| 4794/4794 [03:41<00:00, 21.69it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 21 | Loss 119.50 | Acc 0.5317 | F1 0.5358\n","output_type":"stream"},{"name":"stderr","text":"Epoch 22: 100%|██████████| 4794/4794 [03:42<00:00, 21.57it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 22 | Loss 114.77 | Acc 0.5361 | F1 0.5396\n","output_type":"stream"},{"name":"stderr","text":"Epoch 23: 100%|██████████| 4794/4794 [03:41<00:00, 21.64it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 23 | Loss 111.32 | Acc 0.5425 | F1 0.5461\n","output_type":"stream"},{"name":"stderr","text":"Epoch 24: 100%|██████████| 4794/4794 [03:43<00:00, 21.50it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 24 | Loss 110.42 | Acc 0.5464 | F1 0.5496\n","output_type":"stream"},{"name":"stderr","text":"Epoch 25: 100%|██████████| 4794/4794 [03:42<00:00, 21.53it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 25 | Loss 102.96 | Acc 0.5536 | F1 0.5572\n","output_type":"stream"},{"name":"stderr","text":"Epoch 26: 100%|██████████| 4794/4794 [03:42<00:00, 21.56it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 26 | Loss 103.61 | Acc 0.5543 | F1 0.5574\n","output_type":"stream"},{"name":"stderr","text":"Epoch 27: 100%|██████████| 4794/4794 [03:44<00:00, 21.32it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 27 | Loss 95.40 | Acc 0.5577 | F1 0.5620\n","output_type":"stream"},{"name":"stderr","text":"Epoch 28: 100%|██████████| 4794/4794 [03:48<00:00, 21.02it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 28 | Loss 94.15 | Acc 0.5666 | F1 0.5705\n","output_type":"stream"},{"name":"stderr","text":"Epoch 29: 100%|██████████| 4794/4794 [03:47<00:00, 21.08it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 29 | Loss 94.06 | Acc 0.5665 | F1 0.5699\n","output_type":"stream"},{"name":"stderr","text":"Epoch 30: 100%|██████████| 4794/4794 [03:47<00:00, 21.04it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 30 | Loss 90.71 | Acc 0.5692 | F1 0.5730\n","output_type":"stream"}],"execution_count":53},{"cell_type":"code","source":"# =============================================================\n# 8. STANDARD EVALUATION (SYMMETRY ENFORCED VIA TTA)\n# =============================================================\nprint(\"Loading Best Model for Evaluation with TTA...\")\nmodel.load_state_dict(torch.load(\"best_ddi_model.pt\", map_location=DEVICE))\nmodel.eval()\n\ny_true, y_pred = [], []\n\nwith torch.no_grad():\n    for batch in test_loader:\n        if batch is None: continue\n        g1, g2, y = batch\n        g1, g2 = g1.to(DEVICE), g2.to(DEVICE)\n\n        # Forward Pass 1 (Normal)\n        out1 = model(g1, g2)\n        \n        # Forward Pass 2 (Swapped/Flipped) -> Yeh hai magic trick\n        out2 = model(g2, g1) \n\n        # Average the predictions (Enforcing Symmetry)\n        # Softmax pehle lagana zaroori hai average karne se pehle\n        probs1 = F.softmax(out1, dim=1)\n        probs2 = F.softmax(out2, dim=1)\n        \n        avg_probs = (probs1 + probs2) / 2\n        \n        y_true.extend(y.tolist())\n        y_pred.extend(avg_probs.argmax(dim=1).cpu().tolist())\n\nprint(\"Final Accuracy (Symmetrized):\", accuracy_score(y_true, y_pred))\nprint(\"Final Weighted F1 (Symmetrized):\", f1_score(y_true, y_pred, average=\"weighted\"))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-18T22:15:28.627352Z","iopub.execute_input":"2025-12-18T22:15:28.628085Z","iopub.status.idle":"2025-12-18T22:16:15.181720Z","shell.execute_reply.started":"2025-12-18T22:15:28.628050Z","shell.execute_reply":"2025-12-18T22:16:15.180698Z"}},"outputs":[{"name":"stdout","text":"Loading Best Model for Evaluation with TTA...\nFinal Accuracy (Symmetrized): 0.49384442246700544\nFinal Weighted F1 (Symmetrized): 0.5016178289846266\n","output_type":"stream"}],"execution_count":57},{"cell_type":"code","source":"# =============================================================\n# 9. MC DROPOUT UNCERTAINTY\n# =============================================================\ndef enable_mc_dropout(m):\n    for x in m.modules():\n        if isinstance(x, nn.Dropout): x.train()\n\ndef mc_dropout_predict(model, g1, g2, T=5):\n    model.eval()\n    enable_mc_dropout(model) # Force dropout on\n    probs = []\n    with torch.no_grad():\n        for _ in range(T):\n            # T forward passes with different dropout masks\n            probs.append(F.softmax(model(g1, g2), dim=1).unsqueeze(0))\n    \n    probs = torch.cat(probs, dim=0)\n    mean_probs = probs.mean(dim=0)\n    # Entropy calculation (Uncertainty Metric)\n    entropy = -torch.sum(mean_probs * torch.log(mean_probs + 1e-9), dim=1)\n    return mean_probs, entropy","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-18T22:16:41.766727Z","iopub.execute_input":"2025-12-18T22:16:41.767385Z","iopub.status.idle":"2025-12-18T22:16:41.773223Z","shell.execute_reply.started":"2025-12-18T22:16:41.767354Z","shell.execute_reply":"2025-12-18T22:16:41.772474Z"}},"outputs":[],"execution_count":58},{"cell_type":"code","source":"# =============================================================\n# 10.1 RELIABILITY AUDITOR\n# =============================================================\nimport numpy as np\nfrom xgboost import XGBClassifier\nfrom rdkit.Chem import AllChem\n\ndef fp_batch(smiles):\n    \"\"\"\n    Generate Morgan fingerprints for a list of SMILES.\n    Output shape: (N, 2048)\n    \"\"\"\n    fps = []\n    for s in smiles:\n        mol = Chem.MolFromSmiles(str(s))\n        if mol is None:\n            fps.append(np.zeros(2048))\n        else:\n            fp = AllChem.GetMorganFingerprintAsBitVect(mol, radius=2, nBits=2048)\n            fps.append(np.array(fp))\n    return np.array(fps)\n\n\n# -------------------------------------------------------------\n# 10.2 Prepare Auditor Features (DrugA || DrugB)\n# -------------------------------------------------------------\nprint(\"Generating fingerprints for Auditor...\")\n\nX_train_fp = np.concatenate(\n    [fp_batch(train_df.X1), fp_batch(train_df.X2)], axis=1\n)  # shape: (N, 4096)\n\ny_train_fp = train_df.Y.values\n\nX_test_fp = np.concatenate(\n    [fp_batch(test_df.X1), fp_batch(test_df.X2)], axis=1\n)\n\n\n# -------------------------------------------------------------\n# 10.3 HANDLE MISSING CLASSES (CORRECT WAY: LABEL REMAPPING)\n# -------------------------------------------------------------\n# Auditor is trained ONLY on classes present in training data\naudit_classes = sorted(np.unique(y_train_fp))\nprint(f\"Auditor training on {len(audit_classes)} / {NUM_CLASSES} classes\")\n\n# Create forward & inverse maps\naudit_map = {c: i for i, c in enumerate(audit_classes)}\ninv_audit_map = {i: c for c, i in audit_map.items()}\n\n# Remap labels for auditor training\ny_train_audit = np.array([audit_map[y] for y in y_train_fp])\n\n\n# -------------------------------------------------------------\n# 10.4 Train XGBoost Auditor\n# -------------------------------------------------------------\nprint(\"Training Reliability Auditor (XGBoost)...\")\n\nauditor = XGBClassifier(\n    n_estimators=100,\n    max_depth=6,\n    learning_rate=0.1,\n    objective=\"multi:softprob\",\n    num_class=len(audit_classes),\n    n_jobs=-1,\n    tree_method=\"hist\",\n    random_state=42\n)\n\nauditor.fit(X_train_fp, y_train_audit)\n\nprint(\"Auditor trained successfully (clean & valid).\")\n\n\n# -------------------------------------------------------------\n# 10.5 Auditor Prediction Helper (for Reliability Loop)\n# -------------------------------------------------------------\ndef auditor_predict(fp_batch_slice):\n    \"\"\"\n    Predict original DDI class IDs using the trained auditor.\n    \"\"\"\n    raw_preds = auditor.predict(fp_batch_slice)\n    return np.array([inv_audit_map[p] for p in raw_preds])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-18T22:37:16.148689Z","iopub.execute_input":"2025-12-18T22:37:16.149242Z","iopub.status.idle":"2025-12-18T23:29:57.000368Z","shell.execute_reply.started":"2025-12-18T22:37:16.149218Z","shell.execute_reply":"2025-12-18T23:29:56.999595Z"}},"outputs":[{"name":"stdout","text":"Generating fingerprints for Auditor...\nAuditor training on 74 / 76 classes\nTraining Reliability Auditor (XGBoost)...\nAuditor trained successfully (clean & valid).\n","output_type":"stream"}],"execution_count":63},{"cell_type":"code","source":"# =============================================================\n# 11. RELIABILITY-AWARE AUDITING LOOP \n# =============================================================\n\ny_true_a, y_pred_a, reliab = [], [], []\nptr = 0\n\n# Normalized class frequency from TRAIN set (risk prior)\nclass_freq = train_df.Y.value_counts(normalize=True)\n\nprint(\"Running Reliability Audit...\")\n\nwith torch.no_grad():\n    for batch in tqdm(test_loader, desc=\"Auditing\"):\n        if batch is None:\n            continue\n\n        g1, g2, y = batch\n        g1, g2 = g1.to(DEVICE), g2.to(DEVICE)\n\n        # -----------------------------------------------------\n        # 1. GNN Prediction with MC Dropout (Uncertainty-aware)\n        # -----------------------------------------------------\n        probs, entropy = mc_dropout_predict(model, g1, g2)\n        preds = probs.argmax(dim=1).cpu().numpy()\n\n        # -----------------------------------------------------\n        # 2. Auditor Prediction (USING REMAPPED HELPER)\n        # -----------------------------------------------------\n        bs = len(y)\n\n        # Important: test_loader is NOT shuffled\n        cnt_preds = auditor_predict(X_test_fp[ptr:ptr + bs])\n\n        # -----------------------------------------------------\n        # 3. Reliability Score Computation\n        # -----------------------------------------------------\n        for i in range(bs):\n            # Base confidence from GNN\n            s = probs[i, preds[i]].item()\n\n            # (a) Auditor disagreement penalty\n            if preds[i] != cnt_preds[i]:\n                s *= 0.6\n\n            # (b) Rare-class penalty (clinical risk prior)\n            freq = class_freq.get(preds[i], 0.0)\n            s *= np.exp(-0.5 * (1.0 - freq))\n\n            # (c) Uncertainty penalty (entropy)\n            s *= np.exp(-entropy[i].item())\n\n            reliab.append(s)\n\n        # -----------------------------------------------------\n        # 4. Store outputs\n        # -----------------------------------------------------\n        y_true_a.extend(y.tolist())\n        y_pred_a.extend(preds.tolist())\n\n        ptr += bs\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-18T23:29:57.010719Z","iopub.execute_input":"2025-12-18T23:29:57.011021Z","iopub.status.idle":"2025-12-18T23:31:03.130249Z","shell.execute_reply.started":"2025-12-18T23:29:57.010997Z","shell.execute_reply":"2025-12-18T23:31:03.129337Z"}},"outputs":[{"name":"stdout","text":"Running Reliability Audit...\n","output_type":"stream"},{"name":"stderr","text":"Auditing: 100%|██████████| 917/917 [01:06<00:00, 13.87it/s]\n","output_type":"stream"}],"execution_count":65},{"cell_type":"code","source":"# =============================================================\n# 12. COVERAGE vs ACCURACY (RELIABILITY-BASED REJECTION CURVE)\n# =============================================================\n\nimport numpy as np\nfrom sklearn.metrics import accuracy_score, f1_score\n\n# Convert lists to arrays once (efficiency + clarity)\ny_true_arr = np.array(y_true_a)\ny_pred_arr = np.array(y_pred_a)\nreliab_arr = np.array(reliab)\n\n# Sort samples by reliability (ascending: least reliable first)\norder = np.argsort(reliab_arr)\n\n# Rejection rates: 0% → 50%\nrates = np.linspace(0.0, 0.5, 11)\n\nprint(\"\\nReject% | Coverage% | Accuracy | Weighted F1\")\nprint(\"-\" * 50)\n\nfor r in rates:\n    # Number of samples to reject\n    k = int(len(order) * r)\n\n    # Keep the most reliable samples\n    keep_idx = order[k:]\n\n    # Safety stop: too few samples left\n    if len(keep_idx) < 200:\n        break\n\n    acc = accuracy_score(\n        y_true_arr[keep_idx],\n        y_pred_arr[keep_idx]\n    )\n\n    f1 = f1_score(\n        y_true_arr[keep_idx],\n        y_pred_arr[keep_idx],\n        average=\"weighted\"\n    )\n\n    coverage = len(keep_idx) / len(order) * 100.0\n\n    print(\n        f\"{int(r*100):>3}%    | \"\n        f\"{coverage:6.1f}%    | \"\n        f\"{acc:.4f}   | \"\n        f\"{f1:.4f}\"\n    )\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-18T23:31:03.132250Z","iopub.execute_input":"2025-12-18T23:31:03.132996Z","iopub.status.idle":"2025-12-18T23:31:03.267938Z","shell.execute_reply.started":"2025-12-18T23:31:03.132959Z","shell.execute_reply":"2025-12-18T23:31:03.267319Z"}},"outputs":[{"name":"stdout","text":"\nReject% | Coverage% | Accuracy | Weighted F1\n--------------------------------------------------\n  0%    |  100.0%    | 0.5732   | 0.5748\n  5%    |   95.0%    | 0.5962   | 0.5978\n 10%    |   90.0%    | 0.6179   | 0.6195\n 15%    |   85.0%    | 0.6398   | 0.6408\n 20%    |   80.0%    | 0.6610   | 0.6616\n 25%    |   75.0%    | 0.6793   | 0.6792\n 30%    |   70.0%    | 0.6979   | 0.6975\n 35%    |   65.0%    | 0.7165   | 0.7157\n 40%    |   60.0%    | 0.7352   | 0.7338\n 45%    |   55.0%    | 0.7505   | 0.7486\n 50%    |   50.0%    | 0.7670   | 0.7648\n","output_type":"stream"}],"execution_count":66},{"cell_type":"code","source":"# =============================================================\n# 13. SAVE EVERYTHING (FINAL FIX - KEY SANITIZATION)\n# =============================================================\n\nimport json\nimport joblib\nimport torch\nimport numpy as np\n\n# -------------------------------------------------------------\n# 1. Save Models\n# -------------------------------------------------------------\ntorch.save(model.state_dict(), \"best_ddi_model.pt\")\njoblib.dump(auditor, \"auditor_xgboost.pkl\")\nprint(\"✅ Models Saved.\")\n\n# -------------------------------------------------------------\n# 2. Prepare Metadata with SANITIZED KEYS\n# -------------------------------------------------------------\n\n\ndef clean_dict(d):\n    \"\"\"Converts keys to str and values to python types\"\"\"\n    new_d = {}\n    for k, v in d.items():\n        # Key fix: Force string\n        k_clean = str(k) \n        \n        # Value fix: Handle numpy types\n        if isinstance(v, (np.integer, np.int64)):\n            v_clean = int(v)\n        elif isinstance(v, (np.floating, np.float64, np.float32)):\n            v_clean = float(v)\n        elif isinstance(v, np.ndarray):\n            v_clean = v.tolist()\n        else:\n            v_clean = v\n            \n        new_d[k_clean] = v_clean\n    return new_d\n\n# Manually clean each dictionary\nmetadata = {\n    \"label_map\": clean_dict(label_map),\n    \"class_freq\": clean_dict(class_freq.to_dict()),\n    \"audit_classes\": [int(x) for x in audit_classes], \n    \"audit_map\": clean_dict(audit_map),\n    \"inv_audit_map\": clean_dict(inv_audit_map)\n}\n\n# -------------------------------------------------------------\n# 3. Save JSON \n# -------------------------------------------------------------\nwith open(\"ddi_metadata.json\", \"w\") as f:\n    json.dump(metadata, f, indent=2)\n\nprint(\"Metadata Saved Successfully .\")\n\n\n# -------------------------------------------------------------\n# 4. Final Confirmation & Download\n# -------------------------------------------------------------\nprint(\"\\nALL FILES SAVED:\")\nprint(\"- best_ddi_model.pt\")\nprint(\"- auditor_xgboost.pkl\")\nprint(\"- ddi_metadata.json\")\n\nfrom IPython.display import FileLink\ndisplay(FileLink(\"best_ddi_model.pt\"))\ndisplay(FileLink(\"auditor_xgboost.pkl\"))\ndisplay(FileLink(\"ddi_metadata.json\"))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-18T23:37:33.711565Z","iopub.execute_input":"2025-12-18T23:37:33.712283Z","iopub.status.idle":"2025-12-18T23:37:33.847485Z","shell.execute_reply.started":"2025-12-18T23:37:33.712260Z","shell.execute_reply":"2025-12-18T23:37:33.846926Z"}},"outputs":[{"name":"stdout","text":"✅ Models Saved.\nMetadata Saved Successfully .\n\nALL FILES SAVED:\n- best_ddi_model.pt\n- auditor_xgboost.pkl\n- ddi_metadata.json\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"/kaggle/working/best_ddi_model.pt","text/html":"<a href='best_ddi_model.pt' target='_blank'>best_ddi_model.pt</a><br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"/kaggle/working/auditor_xgboost.pkl","text/html":"<a href='auditor_xgboost.pkl' target='_blank'>auditor_xgboost.pkl</a><br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"/kaggle/working/ddi_metadata.json","text/html":"<a href='ddi_metadata.json' target='_blank'>ddi_metadata.json</a><br>"},"metadata":{}}],"execution_count":70},{"cell_type":"code","source":"# =============================================================\n# 14. AUTO DOWNLOAD TRAINED ASSETS\n# =============================================================\n\nimport os\nfrom IPython.display import FileLink, display\n\nprint(\"Download trained assets:\")\n\nfiles_to_download = [\n    \"best_ddi_model.pt\",\n    \"auditor_xgboost.pkl\",\n    \"ddi_metadata.json\"\n]\n\nfor file in files_to_download:\n    if os.path.exists(file):\n        display(FileLink(file))\n    else:\n        print(f\" File not found: {file}\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-18T23:37:44.396980Z","iopub.execute_input":"2025-12-18T23:37:44.397489Z","iopub.status.idle":"2025-12-18T23:37:44.406051Z","shell.execute_reply.started":"2025-12-18T23:37:44.397470Z","shell.execute_reply":"2025-12-18T23:37:44.405373Z"}},"outputs":[{"name":"stdout","text":"Download trained assets:\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"/kaggle/working/best_ddi_model.pt","text/html":"<a href='best_ddi_model.pt' target='_blank'>best_ddi_model.pt</a><br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"/kaggle/working/auditor_xgboost.pkl","text/html":"<a href='auditor_xgboost.pkl' target='_blank'>auditor_xgboost.pkl</a><br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"/kaggle/working/ddi_metadata.json","text/html":"<a href='ddi_metadata.json' target='_blank'>ddi_metadata.json</a><br>"},"metadata":{}}],"execution_count":71},{"cell_type":"code","source":"import os, json\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom sklearn.metrics import (\n    accuracy_score,\n    f1_score,\n    classification_report,\n    confusion_matrix\n)\nfrom IPython.display import FileLink, display\n\nos.makedirs(\"exports\", exist_ok=True)\n\n# =============================================================\n# 15.1 SAVE FINAL METRICS REPORT\n# =============================================================\nfinal_metrics = {\n    \"accuracy\": accuracy_score(y_true_a, y_pred_a),\n    \"weighted_f1\": f1_score(y_true_a, y_pred_a, average=\"weighted\"),\n    \"total_samples\": len(y_true_a)\n}\n\nwith open(\"exports/final_metrics.json\", \"w\") as f:\n    json.dump(final_metrics, f, indent=2)\n\n# =============================================================\n# 15.2 SAVE CLASSIFICATION REPORT\n# =============================================================\nreport = classification_report(\n    y_true_a, y_pred_a, output_dict=True\n)\npd.DataFrame(report).transpose().to_csv(\n    \"exports/classification_report.csv\"\n)\n\n# =============================================================\n# 15.3 CONFUSION MATRIX (PNG)\n# =============================================================\ncm = confusion_matrix(y_true_a, y_pred_a)\n\nplt.figure(figsize=(10, 8))\nplt.imshow(cm, cmap=\"Blues\")\nplt.title(\"Confusion Matrix (DDI)\")\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"True\")\nplt.colorbar()\nplt.tight_layout()\nplt.savefig(\"exports/confusion_matrix.png\", dpi=300)\nplt.close()\n\n# =============================================================\n# 15.4 COVERAGE vs ACCURACY TABLE (CSV)\n# =============================================================\ncoverage_rows = []\n\ny_true_arr = np.array(y_true_a)\ny_pred_arr = np.array(y_pred_a)\nreliab_arr = np.array(reliab)\norder = np.argsort(reliab_arr)\n\nrates = np.linspace(0, 0.5, 11)\n\nfor r in rates:\n    k = int(len(order) * r)\n    keep = order[k:]\n    if len(keep) < 200:\n        break\n\n    coverage_rows.append({\n        \"reject_percent\": int(r * 100),\n        \"coverage_percent\": len(keep) / len(order) * 100,\n        \"accuracy\": accuracy_score(y_true_arr[keep], y_pred_arr[keep]),\n        \"weighted_f1\": f1_score(y_true_arr[keep], y_pred_arr[keep], average=\"weighted\")\n    })\n\npd.DataFrame(coverage_rows).to_csv(\n    \"exports/coverage_vs_accuracy.csv\", index=False\n)\n\n# =============================================================\n# 15.5 COVERAGE vs F1 GRAPH (PNG)\n# =============================================================\nplt.figure(figsize=(7, 5))\nplt.plot(\n    [r[\"coverage_percent\"] for r in coverage_rows],\n    [r[\"weighted_f1\"] for r in coverage_rows],\n    marker=\"o\"\n)\nplt.xlabel(\"Coverage (%)\")\nplt.ylabel(\"Weighted F1\")\nplt.title(\"Coverage vs Weighted F1\")\nplt.grid(True)\nplt.tight_layout()\nplt.savefig(\"exports/coverage_vs_f1.png\", dpi=300)\nplt.close()\n\n# =============================================================\n# 15.6 RELIABILITY SCORE DISTRIBUTION\n# =============================================================\nplt.figure(figsize=(7, 5))\nplt.hist(reliab_arr, bins=50)\nplt.xlabel(\"Reliability Score\")\nplt.ylabel(\"Count\")\nplt.title(\"Reliability Score Distribution\")\nplt.tight_layout()\nplt.savefig(\"exports/reliability_distribution.png\", dpi=300)\nplt.close()\n\n# =============================================================\n# 15.7 SAVE RAW PREDICTIONS (CSV)\n# =============================================================\npd.DataFrame({\n    \"y_true\": y_true_a,\n    \"y_pred\": y_pred_a,\n    \"reliability\": reliab\n}).to_csv(\"exports/raw_predictions.csv\", index=False)\n\n# =============================================================\n# 15.8 LIST & DOWNLOAD ALL FILES\n# =============================================================\nprint(\"\\n DOWNLOAD ALL EXPORTS:\")\n\nall_files = [\n    \"best_ddi_model.pt\",\n    \"auditor_xgboost.pkl\",\n    \"ddi_metadata.json\",\n    \"exports/final_metrics.json\",\n    \"exports/classification_report.csv\",\n    \"exports/confusion_matrix.png\",\n    \"exports/coverage_vs_accuracy.csv\",\n    \"exports/coverage_vs_f1.png\",\n    \"exports/reliability_distribution.png\",\n    \"exports/raw_predictions.csv\"\n]\n\nfor f in all_files:\n    if os.path.exists(f):\n        display(FileLink(f))\n    else:\n        print(f\" Missing: {f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-18T23:37:52.058428Z","iopub.execute_input":"2025-12-18T23:37:52.058764Z","iopub.status.idle":"2025-12-18T23:37:54.036418Z","shell.execute_reply.started":"2025-12-18T23:37:52.058741Z","shell.execute_reply":"2025-12-18T23:37:54.035689Z"}},"outputs":[{"name":"stdout","text":"\n DOWNLOAD ALL EXPORTS:\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"/kaggle/working/best_ddi_model.pt","text/html":"<a href='best_ddi_model.pt' target='_blank'>best_ddi_model.pt</a><br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"/kaggle/working/auditor_xgboost.pkl","text/html":"<a href='auditor_xgboost.pkl' target='_blank'>auditor_xgboost.pkl</a><br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"/kaggle/working/ddi_metadata.json","text/html":"<a href='ddi_metadata.json' target='_blank'>ddi_metadata.json</a><br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"/kaggle/working/exports/final_metrics.json","text/html":"<a href='exports/final_metrics.json' target='_blank'>exports/final_metrics.json</a><br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"/kaggle/working/exports/classification_report.csv","text/html":"<a href='exports/classification_report.csv' target='_blank'>exports/classification_report.csv</a><br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"/kaggle/working/exports/confusion_matrix.png","text/html":"<a href='exports/confusion_matrix.png' target='_blank'>exports/confusion_matrix.png</a><br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"/kaggle/working/exports/coverage_vs_accuracy.csv","text/html":"<a href='exports/coverage_vs_accuracy.csv' target='_blank'>exports/coverage_vs_accuracy.csv</a><br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"/kaggle/working/exports/coverage_vs_f1.png","text/html":"<a href='exports/coverage_vs_f1.png' target='_blank'>exports/coverage_vs_f1.png</a><br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"/kaggle/working/exports/reliability_distribution.png","text/html":"<a href='exports/reliability_distribution.png' target='_blank'>exports/reliability_distribution.png</a><br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"/kaggle/working/exports/raw_predictions.csv","text/html":"<a href='exports/raw_predictions.csv' target='_blank'>exports/raw_predictions.csv</a><br>"},"metadata":{}}],"execution_count":72},{"cell_type":"code","source":"# =============================================================\n# FINAL COMPREHENSIVE EVALUATION REPORT\n# =============================================================\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nfrom sklearn.metrics import (\n    accuracy_score, f1_score, precision_score, recall_score,\n    top_k_accuracy_score, classification_report\n)\nfrom tqdm import tqdm\n\nprint(\"GENERATING FINAL MEDICAL-GRADE REPORT...\\n\")\n\n# 1. SETUP\n# -------------------------------------------------------------\nmodel.eval()\ny_true = []\ny_pred_hard = []  # Hard class prediction (Argmax)\ny_probs_all = []  # Full probability vectors\ny_reliab = []     # Reliability scores\n\n# Ensure Auditor Data exists (Safety Check)\ntry:\n    if 'X_test_fp' not in locals(): raise NameError\nexcept:\n    print(\"Auditor features missing. Generating on fly (might take 2 mins)...\")\n    X_test_fp = np.concatenate([fp_batch(test_df.X1), fp_batch(test_df.X2)], axis=1)\n\nptr = 0\nclass_freq = train_df.Y.value_counts(normalize=True)\n\n# 2. EVALUATION LOOP (With Symmetry & Reliability)\n# -------------------------------------------------------------\nwith torch.no_grad():\n    for batch in tqdm(test_loader, desc=\"Evaluating\"):\n        if batch is None: continue\n        g1, g2, y = batch\n        g1, g2 = g1.to(DEVICE), g2.to(DEVICE)\n\n        # A. Symmetric Prediction (TTA)\n        out1 = model(g1, g2)\n        out2 = model(g2, g1)\n        \n        # Softmax & Average\n        p1 = F.softmax(out1, dim=1)\n        p2 = F.softmax(out2, dim=1)\n        avg_probs = (p1 + p2) / 2\n        \n        # B. Get Predictions\n        preds = avg_probs.argmax(dim=1).cpu().numpy()\n        probs_np = avg_probs.cpu().numpy()\n        \n        # C. Auditor Check (Reliability)\n        bs = len(y)\n        # Handle Auditor Label Mapping\n        raw_audit_preds = auditor.predict(X_test_fp[ptr:ptr+bs])\n        audit_preds = [inv_audit_map.get(p, -1) for p in raw_audit_preds]\n        \n        # Calculate Reliability Score per Sample\n        batch_reliab = []\n        for i in range(bs):\n            conf = probs_np[i, preds[i]]\n            # 1. Disagreement Penalty\n            if preds[i] != audit_preds[i]: conf *= 0.6\n            # 2. Rare Class Penalty\n            conf *= np.exp(-0.5 * (1 - class_freq.get(preds[i], 0)))\n            batch_reliab.append(conf)\n            \n        # Store Data\n        y_true.extend(y.tolist())\n        y_pred_hard.extend(preds.tolist())\n        y_probs_all.extend(probs_np)\n        y_reliab.extend(batch_reliab)\n        ptr += bs\n\n# Convert to Numpy\ny_true = np.array(y_true)\ny_pred_hard = np.array(y_pred_hard)\ny_probs_all = np.array(y_probs_all)\ny_reliab = np.array(y_reliab)\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"FINAL DDI MODEL REPORT CARD\")\nprint(\"=\"*60)\n\n# 3. BASE METRICS\n# -------------------------------------------------------------\nacc = accuracy_score(y_true, y_pred_hard)\nf1_w = f1_score(y_true, y_pred_hard, average='weighted')\nf1_m = f1_score(y_true, y_pred_hard, average='macro')\nprec = precision_score(y_true, y_pred_hard, average='weighted')\nrec = recall_score(y_true, y_pred_hard, average='weighted')\n\nprint(f\"\\n1. BASE PERFORMANCE (All Samples):\")\nprint(f\"   - Accuracy:          {acc:.4f}  (Base capability)\")\nprint(f\"   - Weighted F1:       {f1_w:.4f}  (Real-world utility)\")\nprint(f\"   - Macro F1:          {f1_m:.4f}  (Rare class handling)\")\nprint(f\"   - Precision:         {prec:.4f}\")\nprint(f\"   - Recall:            {rec:.4f}\")\n\n# 4. TOP-K METRICS (Medical Context)\n# -------------------------------------------------------------\ntop3 = top_k_accuracy_score(y_true, y_probs_all, k=3, labels=np.arange(NUM_CLASSES))\ntop5 = top_k_accuracy_score(y_true, y_probs_all, k=5, labels=np.arange(NUM_CLASSES))\n\nprint(f\"\\n2. DOCTOR ASSISTANT METRICS:\")\nprint(f\"   - Top-3 Accuracy:    {top3:.4f}  (Correct answer in top 3 suggestions)\")\nprint(f\"   - Top-5 Accuracy:    {top5:.4f}  (Correct answer in top 5 suggestions)\")\n\n# 5. RISK-COVERAGE CURVE (The Auditor's Job)\n# -------------------------------------------------------------\nprint(f\"\\n3. RELIABILITY AUDIT (Rejection Curve):\")\nprint(\"   Reject% | Coverage | Accuracy | Risk Reduction\")\nprint(\"   \" + \"-\"*45)\n\nsorted_indices = np.argsort(y_reliab) # Low reliability first\nthresholds = [0, 0.1, 0.2, 0.3, 0.4, 0.5]\n\nfor r in thresholds:\n    # Keep top (1-r)% reliability\n    if r == 0:\n        keep_mask = np.ones_like(y_true, dtype=bool)\n    else:\n        cutoff = int(len(y_true) * r)\n        keep_indices = sorted_indices[cutoff:] # Remove bottom r%\n        keep_mask = np.zeros_like(y_true, dtype=bool)\n        keep_mask[keep_indices] = True\n    \n    subset_acc = accuracy_score(y_true[keep_mask], y_pred_hard[keep_mask])\n    print(f\"   {int(r*100):>3}%    | {int((1-r)*100):>3}%    | {subset_acc:.4f}   | +{(subset_acc-acc)*100:.1f}%\")\n\n# 6. CLASS WISE PERFORMANCE (Short Summary)\n# -------------------------------------------------------------\nprint(f\"\\n4. CLASS DIAGNOSTICS:\")\nreport = classification_report(y_true, y_pred_hard, output_dict=True)\ndf_rep = pd.DataFrame(report).transpose()\n# Filter out 'accuracy', 'macro avg', etc.\ndf_classes = df_rep.iloc[:-3].sort_values(by='f1-score', ascending=False)\n\nbest_3 = df_classes.head(3).index.tolist()\nworst_3 = df_classes[df_classes['support'] > 50].tail(3).index.tolist() # Min support to be fair\n\nprint(f\"   - Best Classes (IDs):  {best_3} (F1 ~ {df_classes.head(3)['f1-score'].mean():.2f})\")\nprint(f\"   - Hardest Classes:     {worst_3} (F1 ~ {df_classes.tail(3)['f1-score'].mean():.2f})\")\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"END OF REPORT\")\nprint(\"=\"*60)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-18T23:44:02.744885Z","iopub.execute_input":"2025-12-18T23:44:02.745510Z","iopub.status.idle":"2025-12-18T23:44:52.606732Z","shell.execute_reply.started":"2025-12-18T23:44:02.745478Z","shell.execute_reply":"2025-12-18T23:44:52.605837Z"}},"outputs":[{"name":"stdout","text":"GENERATING FINAL MEDICAL-GRADE REPORT...\n\n","output_type":"stream"},{"name":"stderr","text":"Evaluating: 100%|██████████| 917/917 [00:49<00:00, 18.49it/s]\n","output_type":"stream"},{"name":"stdout","text":"\n============================================================\nFINAL DDI MODEL REPORT CARD\n============================================================\n\n1. BASE PERFORMANCE (All Samples):\n   - Accuracy:          0.4938  (Base capability)\n   - Weighted F1:       0.5016  (Real-world utility)\n   - Macro F1:          0.3553  (Rare class handling)\n   - Precision:         0.5736\n   - Recall:            0.4938\n\n2. DOCTOR ASSISTANT METRICS:\n   - Top-3 Accuracy:    0.8426  (Correct answer in top 3 suggestions)\n   - Top-5 Accuracy:    0.9245  (Correct answer in top 5 suggestions)\n\n3. RELIABILITY AUDIT (Rejection Curve):\n   Reject% | Coverage | Accuracy | Risk Reduction\n   ---------------------------------------------\n     0%    | 100%    | 0.4938   | +0.0%\n    10%    |  90%    | 0.5356   | +4.2%\n    20%    |  80%    | 0.5835   | +9.0%\n    30%    |  70%    | 0.6397   | +14.6%\n    40%    |  60%    | 0.7080   | +21.4%\n    50%    |  50%    | 0.7758   | +28.2%\n\n4. CLASS DIAGNOSTICS:\n   - Best Classes (IDs):  ['20', '63', '49'] (F1 ~ 0.98)\n   - Hardest Classes:     ['55', '28', '2'] (F1 ~ 0.00)\n\n============================================================\nEND OF REPORT\n============================================================\n","output_type":"stream"}],"execution_count":74},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}